{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb664e2-ce41-4ce8-98dd-dbd9d199a019",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\r\n",
    "can they be mitigated\n",
    "\n",
    "Ans- \n",
    "### Overfitting \n",
    "Overfitting is when a model learns the training data too well , including noise , resulting in poor performance on new data \n",
    "\n",
    "- Consequences : High training accuracy but low test accuracy , poor generalization.\n",
    "\n",
    "- Mitigaion - Simplify the model , use regularzation (L1/L2) ,cross-validation, early stopping ,and pruning (for decion trees)\n",
    "\n",
    "\n",
    "### Underfitting \n",
    "Underfitting is when a model is too simple to capture data patterns ,leading to poor performance on both training and test sets.\n",
    "\n",
    "- Consequences : Low accuracy , fails to capture key trends.\n",
    "\n",
    "- Mitigation : Increase model complexity , enhances features , reduce regularization , and train longer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcbae6-3d47-4119-b775-1b15858b0427",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief\n",
    "\n",
    "Ans- To reduce overfitting in machine learning model , you can \n",
    "\n",
    "- Simplify the model : Use a less complex model with fewer parameters to prevent it from capturing noise\n",
    "\n",
    "- Regularization : Apply L1 (Lasso or L2(Ridge) regularization to penalize  large weights and reduce model complexity\n",
    "\n",
    "- Cross-Validation : Implement k fold cross - validation  to ensure the model 's performance is consistent across different data splits\n",
    "\n",
    "- Early Stopping : Stop training when perforamce on the validation set starts to decline\n",
    "\n",
    "- Data Argumentaion: Increase the size of the training data by argument it to help the model generalize better\n",
    "\n",
    "- Purning (For trees) : Remove less important branches to simplify the model\n",
    "\n",
    "- Dropout( For Neural Networks) : Randomly drop units during training to prevent co-adoptation of neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fce66f-60cc-4224-91c8-f15edcf80c42",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML\n",
    "\n",
    "Ans- Underfitting happens when a model is too simple to learn pattern in the data , leading to poor performance on both training  and test sets.\n",
    "\n",
    "### When Underfitting Can Happen \n",
    "\n",
    "- Too Simple Model : Using a basic model (e.g., Linear regression) for complex data\n",
    "\n",
    "- Not Enough Training : Stopping  training  too early\n",
    "\n",
    "- Too much Regularization : Applying strong L1/L2 regularization that limits learning\n",
    "\n",
    "- Few Features : Using data that lacks enough relevant features .\n",
    "\n",
    "- Low Model Complexity : Choosing  a model with parameter (e.g., shallow decision trees)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb5cdd-1d07-4ff1-9f93-44e1fcbc57a3",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "\n",
    "Ans - \n",
    "\n",
    "The bias-variance tradeoff is about finding the right balance between two type of errors in a model :\n",
    "\n",
    "- Bias : The error from making a model too simple. This leads to underfitting, where the model misses important patterns and performs poorely on both training and test data\n",
    "\n",
    "- Variance : The error from making a model is too complex. This leads to overfitting, where the model learns the noise in the training data too well and doesn't perform well on new data\n",
    "\n",
    "\n",
    "Relationship\n",
    "\n",
    "- High Bias = Simple model , Underfits data\n",
    "- High Variance = Complex model , overfits data\n",
    "\n",
    "\n",
    "A good model has the right mix of bias and variance , performing well on both training  and new data. This balance helps the model generalize effectively "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2739192-0437-4c87-a905-8c6a27ec2c82",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "Ans - \n",
    "\n",
    "1. Check Performance on Train and test Data \n",
    "\n",
    "- Overfitting : High training accuracy but low test/validation accuracy\n",
    "\n",
    "- Underfitting : Low accuracy on both training and test/validation data\n",
    "\n",
    "2. Look at Learning Curves :\n",
    "\n",
    "- Overfitting : Training error is low , but validation error is High\n",
    "\n",
    "- Underfitting : Both training and validation errors are high\n",
    "\n",
    "3. Use Cross-Validation:\n",
    "\n",
    "- Run  k- fold cross-validation . if training accuracy is much higher than cross-validation accuracy , its a sign of overfitting\n",
    "\n",
    "4. Add Regulaization:\n",
    "\n",
    "- if adding regularization (e.g., L1/L2) impoves the test accuracy , the model was likely overfitting\n",
    "\n",
    "5. Monitor During Training :\n",
    "\n",
    "- If validation accuracy stops improving while training accuracy continues to rise , Overfitting is likly happening \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15717b27-147c-451f-bd14-0b9ebf04ed76",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "\n",
    "Ans - \n",
    "Bias and variance  are two source of errors that affect a model's performance :\n",
    "\n",
    "Bias \n",
    "- Refer to errors due to over simplistic models that make strong assumptions.\n",
    "- Lead to underfitting : the model misses important data patterns\n",
    "- Example : Linear regression used on complex , non- linear data.\n",
    "- Performance : Poor on both training and teat data\n",
    "\n",
    "Variance \n",
    "\n",
    "- Refers to errors due to overly complex models that capture noise in the training data\n",
    "- Leads to Overfitting : the model adapts too closely to the training data and fails to generalize\n",
    "- Example : A deep decision tree with many branches\n",
    "- Performance : High accuracy on training data , poor on test data\n",
    "\n",
    "Key Difference \n",
    "\n",
    "High bias model \n",
    "- Simple models (e.g., linear regression , shallow tree)\n",
    "- Consistent errors on both training and test data\n",
    "- Result : Underfitting\n",
    "\n",
    "High Vairance Models :\n",
    "- Complex models (e.g., deep neural networks ,complex decision trees)\n",
    "- Very good performance on training data but fails on test data\n",
    "- Result : Overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e41dc-168b-4d0e-bab8-7c4c272f1da4",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.\n",
    "\n",
    "Ans- What is Regularization in Simple Terms?\n",
    "Regularization helps prevent a machine learning model from overfitting (being too complex and fitting the training data too closely). It does this by adding a penalty to the model to keep it simpler and more general.\n",
    "\n",
    "How Regularization Helps:\n",
    "It stops the model from using very large weights, which can make it too sensitive to the training data.\n",
    "This helps the model work better on new, unseen data.\n",
    "Common Regularization Techniques:\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "Adds a penalty based on the absolute values of the weights.\n",
    "Effect: Some weights become zero, which helps choose only the most important features.\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "Adds a penalty based on the squared values of the weights.\n",
    "Effect: Makes all weights smaller but doesn’t reduce them to zero, balancing the model’s complexity.\n",
    "Elastic Net:\n",
    "\n",
    "Combines L1 and L2 regularization.\n",
    "Effect: Gives a mix of feature selection (L1) and weight shrinking (L2).\n",
    "How it Works:\n",
    "A parameter called λ (lambda) controls the penalty strength. A higher λ means more regularization, leading to simpler models. Too high a value can cause underfitting.\n",
    "Why Use Regularization?\n",
    "Prevents Overfitting: Keeps the model from memorizing the training data.\n",
    "Improves Generalization: Makes the model perform better on new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
